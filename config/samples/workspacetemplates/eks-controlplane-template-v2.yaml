# EKS Control Plane WorkspaceTemplate
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: WorkspaceTemplate
metadata:
  name: eks-controlplane-template-v2
  namespace: default
spec:
  template:
    metadata:
      description: "Template for creating EKS Control Plane"
      version: "1.0.0"
      tags:
        provider: "aws"
        resource: "eks"
        environment: "dev"
    spec:
      writeConnectionSecretToRef:
        name: "${WORKSPACE_NAME}-eks-connection"
        namespace: default
      providerConfigRef:
        name: aws-provider-for-eks
      forProvider:
        source: Inline
        enableTerraformCLILogging: true
        env:
          - name: HELM_REPOSITORY_CACHE
            value: /tmp/.helmcache
        vars:
          - key: cluster_name
            value: "demo-cluster"
          - key: kubernetes_version
            value: "1.31"
        varFiles:
          - source: SecretKey
            format: HCL
            secretKeyRef:
              namespace: default
              name: "${WORKSPACE_NAME}-vpc-connection"
              key: vpc_config
        module: |
          locals {
            tags = {
              Environment = "dev"
              Terraform   = "true"
            }
          }

          module "kms" {
            source  = "terraform-aws-modules/kms/aws"
            version = "~> 2.1"

            description           = "${var.cluster_name} cluster encryption key"
            enable_default_policy = true
            key_owners            = [data.aws_caller_identity.current.arn]

            tags = local.tags
          }

          module "eks" {
            source  = "terraform-aws-modules/eks/aws"
            version = "~> 20.29"

            cluster_name                   = var.cluster_name
            cluster_version                = var.kubernetes_version
            cluster_endpoint_public_access = true

            vpc_id     = var.vpc_id
            subnet_ids = var.private_subnets

            # Security groups are managed separately
            create_cluster_security_group = false
            create_node_security_group    = false

            enable_cluster_creator_admin_permissions = true

            create_kms_key = false
            cluster_encryption_config = {
              resources        = ["secrets"]
              provider_key_arn = module.kms.key_arn
            }

            # Default Fargate profiles
            fargate_profiles = {
              karpenter = {
                selectors = [
                  { namespace = "karpenter" }
                ]
              }
              kube_system = {
                name = "kube-system"
                selectors = [
                  { namespace = "kube-system" }
                ]
              }
            }

            tags = merge(local.tags, {
              "karpenter.sh/discovery" = var.cluster_name
            })
          }

          module "eks_blueprints_addons" {
            source  = "aws-ia/eks-blueprints-addons/aws"
            version = "~> 1.16"

            cluster_name      = module.eks.cluster_name
            cluster_endpoint  = module.eks.cluster_endpoint
            cluster_version   = module.eks.cluster_version
            oidc_provider_arn = module.eks.oidc_provider_arn

            # Wait for Fargate profiles to be ready
            create_delay_dependencies = [for prof in module.eks.fargate_profiles : prof.fargate_profile_arn]

            eks_addons = {
              coredns = {
                configuration_values = jsonencode({
                  computeType = "Fargate"
                  resources = {
                    limits = {
                      cpu    = "0.25"
                      memory = "256M"
                    }
                    requests = {
                      cpu    = "0.25"
                      memory = "256M"
                    }
                  }
                })
              }
              vpc-cni = {
                configuration_values = jsonencode({
                  resources = {
                    requests = {
                      cpu    = "0.1"
                      memory = "128M"
                    }
                  }
                })
              }
              kube-proxy = {
                configuration_values = jsonencode({
                  resources = {
                    requests = {
                      cpu    = "0.1"
                      memory = "64M"
                    }
                  }
                })
              }
            }

            enable_karpenter = true

            karpenter = {
              helm_config = {
                cacheDir = "/tmp/.helmcache"
              }
            }

            karpenter_node = {
              iam_role_use_name_prefix = false
            }

            tags = local.tags
          }

          resource "aws_eks_access_entry" "karpenter_node_access_entry" {
            cluster_name      = module.eks.cluster_name
            principal_arn     = module.eks_blueprints_addons.karpenter.node_iam_role_arn
            kubernetes_groups = []
            type              = "EC2_LINUX"

            lifecycle {
              ignore_changes = [
                kubernetes_groups
              ]
            }
          }


          module "karpenter" {
            source  = "terraform-aws-modules/eks/aws//modules/karpenter"
            version = "~> 20.29"

            cluster_name          = module.eks.cluster_name
            enable_v1_permissions = true
            namespace             = "karpenter"

            # Name needs to match role name passed to the EC2NodeClass
            node_iam_role_use_name_prefix = false
            node_iam_role_name            = "${module.eks.cluster_name}-node"

            # EKS Fargate does not support pod identity
            create_pod_identity_association = false
            enable_irsa                     = true
            irsa_oidc_provider_arn          = module.eks.oidc_provider_arn

            tags = local.tags
          }

          resource "helm_release" "karpenter" {
            name             = "karpenter"
            namespace        = "karpenter"
            create_namespace = true
            repository       = "oci://public.ecr.aws/karpenter"
            chart            = "karpenter"
            version          = "1.0.7"
            wait             = false

            values = [
              <<-EOT
              dnsPolicy: Default
              settings:
                clusterName: ${module.eks.cluster_name}
                clusterEndpoint: ${module.eks.cluster_endpoint}
                interruptionQueue: ${module.karpenter.queue_name}
              serviceAccount:
                annotations:
                  eks.amazonaws.com/role-arn: ${module.karpenter.iam_role_arn}
              webhook:
                enabled: false
              EOT
            ]
          }

          resource "kubernetes_manifest" "ec2_node_class" {
            manifest = {
              apiVersion = "karpenter.k8s.aws/v1"
              kind       = "EC2NodeClass"
              metadata = {
                name = "default"
              }
              spec = {
                amiSelectorTerms = [
                  {
                    alias = "bottlerocket@latest"
                  }
                ]
                role = "${module.eks.cluster_name}-node"
                subnetSelectorTerms = [
                  {
                    tags = {
                      "karpenter.sh/discovery" = "${module.eks.cluster_name}"
                    }
                  }
                ]
                securityGroupSelectorTerms = [
                  {
                    tags = {
                      "karpenter.sh/discovery" = "${module.eks.cluster_name}"
                    }
                  }
                ]
                tags = {
                  "karpenter.sh/discovery" = "${module.eks.cluster_name}"
                }
              }
            }

            depends_on = [resource.helm_release.karpenter]
          }

          resource "kubernetes_manifest" "node_pool" {
            manifest = {
              apiVersion = "karpenter.sh/v1"
              kind       = "NodePool"
              metadata = {
                name = "default"
              }
              spec = {
                template = {
                  spec = {
                    nodeClassRef = {
                      group = "karpenter.k8s.aws"
                      kind  = "EC2NodeClass"
                      name  = "default"
                    }
                    requirements = [
                      {
                        key      = "karpenter.k8s.aws/instance-category"
                        operator = "In"
                        values   = ["c", "m", "r"]
                      },
                      {
                        key      = "karpenter.k8s.aws/instance-cpu"
                        operator = "In"
                        values   = ["4", "8", "16", "32"]
                      },
                      {
                        key      = "karpenter.k8s.aws/instance-hypervisor"
                        operator = "In"
                        values   = ["nitro"]
                      },
                      {
                        key      = "karpenter.k8s.aws/instance-generation"
                        operator = "Gt"
                        values   = ["2"]
                      }
                    ]
                  }
                }
                limits = {
                  cpu = 1000
                }
                disruption = {
                  consolidationPolicy = "WhenEmpty"
                  consolidateAfter    = "30s"
                }
              }
            }

            depends_on = [kubernetes_manifest.ec2_node_class]
          }

          variable "region" {
            type        = string
            description = "Name of AWS Region"
          }

          variable "cluster_name" {
            type        = string
            description = "Name of the EKS cluster"
          }

          variable "kubernetes_version" {
            type        = string
            description = "Kubernetes version for the EKS cluster"
          }

          variable "vpc_id" {
            type        = string
            description = "ID of the VPC where EKS cluster will be created"
          }

          variable "private_subnets" {
            type        = list(string)
            description = "List of private subnet IDs for EKS cluster"
          }

          output "cluster_endpoint" {
            description = "Endpoint for EKS control plane"
            value       = module.eks.cluster_endpoint
          }

          output "cluster_name" {
            description = "The name of the EKS cluster"
            value       = module.eks.cluster_name
          }

          output "cluster_certificate_authority_data" {
            description = "Base64 encoded certificate data required to communicate with the cluster"
            value       = module.eks.cluster_certificate_authority_data
            sensitive   = true
          }

          output "oidc_provider" {
            description = "The OpenID Connect identity provider (issuer URL without leading https://)"
            value       = module.eks.oidc_provider
          }

          output "oidc_provider_arn" {
            description = "The ARN of the OIDC Provider"
            value       = module.eks.oidc_provider_arn
          }

          output "pod_execution_role_arn" {
            description = "The ARN of the pod execution role for Fargate profiles"
            value       = module.eks.fargate_profiles.pod_execution_role_arn
            sensitive   = true
          }

          output "kubeconfig" {
            description = "Kubeconfig in YAML format"
            sensitive   = true
            value       = <<-EOT
            apiVersion: v1
            clusters:
            - cluster:
                certificate-authority-data: ${module.eks.cluster_certificate_authority_data}
                server: ${module.eks.cluster_endpoint}
              name: ${module.eks.cluster_name}.ap-northeast-1.eksctl.io
            contexts:
            - context:
                cluster: ${module.eks.cluster_name}.ap-northeast-1.eksctl.io
                user: rancher-installer@${module.eks.cluster_name}.ap-northeast-1.eksctl.io
              name: rancher-installer@${module.eks.cluster_name}.ap-northeast-1.eksctl.io
            current-context: rancher-installer@${module.eks.cluster_name}.ap-northeast-1.eksctl.io
            kind: Config
            preferences: {}
            users:
            - name: rancher-installer@${module.eks.cluster_name}.ap-northeast-1.eksctl.io
              user:
                exec:
                  apiVersion: client.authentication.k8s.io/v1beta1
                  args:
                  - eks
                  - get-token
                  - --output
                  - json
                  - --cluster-name
                  - ${module.eks.cluster_name}
                  - --region
                  - ${var.region}
                  command: aws
                  env:
                  - name: AWS_STS_REGIONAL_ENDPOINTS
                    value: regional
                  provideClusterInfo: false
            EOT
          }
